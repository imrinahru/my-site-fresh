---
title: Designing the Ground Where AI Grows
date: 2025-06-29
tags:
  - essay
summary: AI hype is only a phenomenon embedded in current worrisome AI-centric context. Some thoughts on how it can be redefined.
draft: false
cover:
---
![cover_newscientist_tomgauld.webp]({{ '/content/assets/img/cover_newscientist_tomgauld.webp' | relative_url }})


## Context  

There has been a rise in what I saw as a drive towards "AI first". LLMs are starting to becoming the norm, people start to agitate towards either, the expanded application of LLMs in business contexts, or, shifting from LLMs for other models in pursuit of more advanced artificial intelligence (continuous learning/open-ended revolution, AGI).  I'm interested in unpacking the discomfort I feel toward this trend, and uncover insights that might point toward a different, or supplementary approach to AI development.


> There won't be enough people, so we need AI to fill up.
> Eduardo Gonzalez, Founder of Xpress.ai, an AI startup aiming to provide digital workforces for enterprise

> (...) one years, two years in future half 50% AI do. AIs provide new resource to client. Don't' notice client. (correction: Clients don't notice)
> Human have mistake. AI is a not have mistake. We can train AI and not do a same mistake. AI can't lie.
> [How AI employees are solving Japan's labor shortage](https://www.disruptingjapan.com/one-japanese-companys-deployment-of-ai-employees/)

> - **AI Scientists**: who keep exploring and making discoveries around the clock
> - **AI Artists**: who create new kinds of music, art, and stories we've never seen before
> - **AI Engineers**: who design buildings, spaceships, and even new types of computers
> - **AI Teachers**: who help every student learn in the way that works best for them
> - **AI Doctors**: who find new treatments and maybe even help people live longer
> [The future of AI is open-ended](https://richardcsuwandi.github.io/blog/2025/open-endedness/?utm_source=chatgpt.com)



## What's the problem?

I admire the pure scientific curiosity, and if I'm in the shoes of these engineers and investors, I can imagine myself having both the capacity, resources, and appetite to work towards an increasingly powerful intelligence. However, it also reminded me of the moments when human inventions have step-by-step led ourselves to the current state of no return. 

It's like a delicate symbiosis of nature. If gone too far, mutualism would turn into a form of parasitism, the transition is out-of-sight yet exponential. It would be too late to notice when the host already ceded its dominant seat (more likely, the host may no longer have the capacity to notice at that stage). This should roughly correlates with the concept of singularity, but I would assume many of us find it hard to grasp. It is indeed beyond our capacity to recognize in real time, neither the watershed moment, nor the scale of the impact. Think about Capitalism. Do you still have the ability to imagine a world without it?

So how should we avoid turning on the "machine of no-return" to find out we have increased the misery instead of the flourishing? 

One thing is to increase the diverse population who are confident about and have ready access to the technology in question, and importantly, have a say and involvement in how they are designed. This is easier said than done. But the ethos is shared by the expansion of personal computers and digital fabrication movement.

Another aspect is even more difficult to enforce, since it's saying no to the temptation for unlimited optimization and advancement of AI tools. The reference here is Illich's _Tools for Conviviality_. 

> Each environment has a corresponding set of naturally efficient tools. A tool is efficient when it is appropriate to the scale of the operator and to the limits within which this operator can act with autonomy. Tools that require more time, space, energy, or authority than is within the competence of the user are, by definition, non-convivial."
> Illich, Tools for Conviviality

Self-limiting is difficult, in the sense that nobody can legitimize the stopping of scientific research activities, including that of AI. However, once the more powerful intelligence is presented, people cannot refuse but use it, both on the individual and the societal level. Also, pure scientific research is hard to find these days. The academic institution is largely intertwined with the Capitalist system and has a strong interdependent relationship with the private sector. With abundant resources, many scientific research are also generated from the private sector.

Then, how should we approach?


## Designing the tools + context of AI (or rather, tools = context) 

Current AI space inherits the most distinct dichotomy we have created through industrialization: the consumer-designer relationship. Much of the software and hardware tools surrounding us are already too complex to grasp quickly and fully. At one time, calm technology, or stealth technology, is thought to be one of the solutions for this complexity, though exacerbating the division between the user and the designer.

The problem with this dichotomy is that there's only a handful of designers making the most important decisions for the thousands and millions of anonymous users who may spend more time dealing with the tool than the actual designer who devised it. How irrational and isolating that experience is!

One of the revelation I had when I learnt creative coding for the first time was profound. As an in-house designer who had only known Adobe creative suites as "the software" at the time, the openness and clarity of the Processing and later p5.js were eye-opening. Looking back, I'm no longer able to ignore the frustration I feel towards Adobe, in losing my time not on the designing itself, but on grappling with a hidden function, an arbitrary interface design that may soon be updated. I wouldn't ignore the difficulty of learning a new coding language, but I see that effort more as a necessary experience to make it my own.

How to apply this liberating experience to AI tools?

There are some criteria that can be derived from this analogy:

- the tools can be modified by the user
- the tools do not coerce dependence
- the tools do not eliminate other options
- the tools allow creative autonomy
- the tools allow co-creation among different parties
- the tools represent the larger ethos of openness, and freedom

In addition, I would like to emphasize the importance of below points in the current global landscape:

- the tools connect people with different backgrounds around the world
- the tools allow humans to understand the wider environment that's not human-centric

Here, we can start to see that, designing the tool is far more than designing the tool itself. Inheriting the perspective of materialist philosophy, and from what we've observed so far, tools and the context in which they are used cannot be separated. Designing a tool is also designing (or reinforcing) the context in which it is used.

In this line of thought, the designer = user will likely not develop the tool like an in-house designer doing regulated sprints. Instead, the value the designer can provide resides not only in the product itself, but also in the interactive process and journey as a user, and a community one can generate along the way. If a supportive community can luckily take off, then the product can indeed be self-supportive and self-generating. But it's better to think of this autonomous ecosystem as a bonus stage to keep the mental sanity, as communities involve others. What's important is to start with one's own interactive journey that truly bring joy and satisfaction. Others may join along the way, especially if it's open and fun.


## Make use of HCD


The advancement of technology does not inherently bring about the negative impact on humanity, it is how they are designed, and how they are used that matter. And how they are provided and used are shaped in the macro context that I have outlined above. Even in the profit-driven business practices, we managed to accumulate some empirical know-hows on how to design tools so that they empower people, and hopefully also the whole planet.

I want to refer to Ben Schneiderman's _Designing the User Interface_ (1986, 2016), and more recent _Human-Centered AI_ (2022). His message is clear throughout, that tools should be designed to empower people. The idea has involved from balancing automation vs. human control, to ensuring human control while increasing automation. He proposed HCAI Framework, suggesting that there is a sweet spot before excessive automation or excessive human control, the are where the tool is reliable, safe, and feels like an extension of oneself, like digital cameras.


![HCAIframework.png]({{ '/content/assets/img/HCAIframework.png' | relative_url}})

In order to hit that sweet spot, we need to understand user needs, understand user actions, sometimes guardrails and interlocks are necessary for safety reasons, but it should not undermine human control, and also the sense of being in control. 

Current commonly circulated design metaphors of AI is therefor missing the point, and sometimes dangerously pushing people to give up their autonomy. Instead of thinking:

- Intelligent agent, thinking machine, cognitive actor;
- teammate, collaborator, partner, companion;
- Autonomous actor (independent, self-directed);
- Social robot (anthropomorphic, humanoid)

They can be framed as:

- Super powers (that augments human abilities, like navigation choices);
- Tele-actions (dexterous, instrument, powerful, prosthetic, like rovers);
- Control centers (human oversight, supervisory control, like traffic lights);
- Active appliances (consumer-oriented, low cost, easy to use)

The AI art-making is a rampant example of human serving the AI tools. AI tools like Mid-journey is so hard to use, that artists would take hundreds and thousands of trials during months-long period. However, the human effort is somewhat eliminated from the public discussion and is hidden behind the powerful aura of AI.


## Conclusion

The current AI hype and AI-centric narrative dominating the public realm is concerning. The game of AI development is skewed in the first place that resulted in a few people controlling most of the development resources. The few people are also the ones who push forward the "AGI" narrative that's starting to shape how people perceive and use AI in a harmful way. 

I do not disagree that some of the platforms like Hugging Face is democratizing some of the AI tools to the limited public, in a limited way. But in comparison to creative coding community that's been around since 2000s, I feel there are so many we can do, starting from demystifying AI. We've already seen the high ceiling it can provide, but we also need to ensure low floor in the first place, so that anybody with the intention can have a meaningful involvement in the using and making of the tool. Meaningful involvement is not getting people to be an anonymous user of a paid model, it is lowering the bar so that people can touch the most fundamental part of the AI technology, understanding how it can be used, misused, and modified to fit "our" needs. I think in the future, if we can make the satisficing tools available for everybody at a low cost, people will have the power to refuse the unnecessarily powerful model that the growth-driven companies are trying to sell to us.

So an educational program, platform, and community are likely one of the important elements in demystifying the AI technology. It helps to reshape the unhealthy narrative that we have now. 

Another thing that I'd love to try is to rethink current agentic designs, especially through the HCD lens. The purpose here is not merely improving user interaction with AI agent, but in a meta-level, redefining what is the best design in reference to the human values (rights, justice, dignity), and individual goals (self-efficacy, creativity, responsibility, social connections, empowerment). In this sense, the AI model may not even need to be a big model, or LLM, some other model design can be considered from ground up. 
