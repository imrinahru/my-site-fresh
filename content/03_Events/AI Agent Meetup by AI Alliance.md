---
title: 2025-06-26 — Untitled 1
date: 2025-06-26
tags:
  - event
location: 
draft: true
summary: |
  Key takeaway in 20 words.
---

## Overview  
- People fighting over API domination is now fighting for the protocol domination
- NLIP can act as an interface facing human, as well as between agents
- About multi-agent framework, from Xpress AI, Inc: Agent Operating System for the enterprise
- There won't be enough people, so we need AI
- Agent based automation is only a workflw, it
- Install desktop app for AI to aceess other apps
- Bot that takes notes
- Perceive the env, learn from the interaction (currently not)
- execution AI + tool and environment can work, until
- LLMs suffer from a "sensory overload"
- When the context window gets larger, it becomes less accurate, since itOutside where it was trained
- Vector database is onyl superficial knowledge, need to fintune the weights in order to know where the information is in the first place
- Needs:
	- has ab self identity
	- learns from feedback (not data training)
	- Get the entire job done (multi-step process)
	- Xaibo 
	- event-based system, the only framework that can finetune itself
- debugbility
- Wait for a human to approve it, for things that 
- train the agent to how to do it right, it starts to frail, the longer the context window, 
- the dumber, keep the feedback loop is the best way to train
- Diffusion model is a source of hint. It has lots of token all at once, just like human brain, people will have all different thoughts all at once and unconsciously pick one
- The agent contains multiple-agents
- LLM is not AI, they are dumber than 4year old in games
- We evaluate AI in terms of memory test, that human cannot do
- When you have desktop app for the AI, the AI may know how to navigate, but it sometimes fails when you need to stop and think 
- in-context learning is ok, but in deep learning, only using normal data 
- Short-term memory is possible in the context-window
- long-term learning = weights of the model, we don't know how much you can take that? Do we need to train from zero, 
- text to LoRA, it skips the fine-tuning stuff, that's an interesting case
- Dream, people learn from dream, how can do that in AI? automate, the training in a dream, the old llm, past ngiht, a new llm
- We want a memory system, that doesn't exist now, the system that can locate from database 
- How to onbard the essence?

- Match the word in the document for search, search engine result become gamified
- Insted of looking at the page, people created links, index the links Which page has the most links point to it?
- Which word is next to which word, some trillions of tokens are needed
- That's a math work, what's the most likely next word in the Internet? pass the whole thing, then keeps going,
- A static model containing math that's answering question, it takes so much time that it only gives at chunks, which makes it look even more human
- Static model cannot answer information they have never seen, in it, it hallucinates (add a bit variable, it doesn't give the exact same answer everytime, it looks more human), but it then also gives some answers wrong
- You can take your data and add to vectore database, LLM is storing that data in a simiar way,
- use a RAG archi, you can ask and get the answer if it's in the data you provided

Data prepkit, clean up data 
	 gives url, up to 100 pages, grap the contents, cleans up all the data, use Dokling tochange to markdown, chunks it up, number of word/sentences/ then vectorize them
	 AI Alliance Github repo AllyCat, it's not an agent? no, it's a RAG, to change it to Agentic RAG
	 Can create more versions, benchmarks, to have reinformament learning, to create loop
## Key takeaways  
- …  
- …

## Slides & media  
- ![Slide thumbnail](./img/slide.png)  
- Recording: <https://youtu.be/…>  

## Related notes  
- [[Projects/Citizen’s Audit Toolkit]]
